# Earthback — LoRA Training Pipeline

This folder contains everything needed to train 12 character LoRAs using Kohya SS and Flux1-dev-fp8.

---

## Status

| Step | Status |
|------|--------|
| T1/T2/T3 images (1 per char) | ✅ Done — in comfyui-output/ |
| T4 images (15 per char target) | ✅ Queuing — will complete ~2-3 hrs after last queue run |
| Dataset prep (prepare-dataset.py) | ⬜ Run after images land |
| Kohya SS install | ⬜ See install section below |
| James Osei LoRA (train first) | ⬜ |
| All 12 character LoRAs | ⬜ |

---

## Folder Structure

```
lora-training/
├── datasets/
│   ├── james-osei/
│   │   └── images/          ← prepare-dataset.py copies images + .txt captions here
│   ├── lena-hartmann/
│   │   └── images/
│   └── ... (12 characters total)
├── configs/
│   ├── flux-lora-base.toml      ← master template
│   └── flux-lora-[char].toml    ← generated by generate-train-bats.py
├── output/
│   └── james-osei-v1.safetensors  ← trained LoRAs land here
├── logs/                           ← TensorBoard logs
└── scripts/
    ├── prepare-dataset.py          ← step 1: builds dataset from comfyui-output
    └── generate-train-bats.py      ← step 2: makes per-char train .bat files
```

---

## Step 1 — Prepare Dataset

After images finish generating, run from PowerShell or cmd:

```powershell
cd C:\users\adrxi\Earthback\lora-training\scripts
python prepare-dataset.py
```

This will:
- Scan `comfyui-output/` for all character images
- Copy them into `datasets/<char>/images/`
- Write matching `.txt` caption files in Kohya SS format

**Options:**
```
python prepare-dataset.py --dry-run          # preview without copying
python prepare-dataset.py --char james-osei  # one character only
python prepare-dataset.py --min 15           # only chars with 15+ images
```

**Expected image counts per character:**
- T1 = 1 (canonical portrait)
- T2 = 1 (action shot)
- T3 = 1 (environmental)
- T4-01 through T4-05, 3 passes each = 15 T4 images
- **Total: ~18 images per character** ✓

---

## Step 2 — Install Kohya SS (if not already installed)

```powershell
cd D:\AI
git clone https://github.com/bmaltais/kohya_ss.git
cd kohya_ss
python -m venv venv
venv\Scripts\activate
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130
pip install -r requirements.txt
```

**Note:** Use the same CUDA version (cu130) as your ComfyUI install to share the PyTorch cache.

Kohya SS GUI (optional):
```powershell
cd D:\AI\kohya_ss
venv\Scripts\activate
python gui.py
```

---

## Step 3 — Generate Per-Character Configs

```powershell
cd C:\users\adrxi\Earthback\lora-training\scripts
python generate-train-bats.py
```

This creates:
- `configs/flux-lora-james-osei.toml` (and 11 more)
- `train-james-osei.bat` (and 11 more) in `lora-training/`

**Before running:** Open `configs/flux-lora-base.toml` and confirm these paths match your system:
```toml
pretrained_model_name_or_path = "D:/AI/models/flux1-dev-fp8.safetensors"
clip_l  = "D:/AI/models/clip_l.safetensors"
t5xxl   = "D:/AI/models/t5xxl_fp16.safetensors"
ae      = "D:/AI/models/ae.safetensors"
```

---

## Step 4 — Train (start with James Osei)

```powershell
cd C:\users\adrxi\Earthback\lora-training
train-james-osei.bat
```

Training takes approximately **20-30 minutes per character** on RTX-class GPU with fp8 model.

The trained LoRA lands at: `output/james-osei-v1.safetensors`

---

## Step 5 — Test LoRA in ComfyUI

1. Copy `output/james-osei-v1.safetensors` to your ComfyUI LoRA models folder
2. In ComfyUI, add a LoRA Loader node between the model and the sampler
3. Set strength to 0.8–1.0
4. Use trigger word in positive prompt: `JAMES_OSEI_EB`

Test prompt:
```
Documentary photograph, 35mm film. JAMES_OSEI_EB, standing outside a hempcrete construction site in Kumasi Ghana, West African afternoon light, confident expression. Photojournalism style.
```

---

## Training Config Notes

| Parameter | Value | Reason |
|-----------|-------|--------|
| Network rank (dim) | 16 | Good detail capture / reasonable file size |
| Network alpha | 16 | Same as rank = full-rank alpha (standard) |
| Learning rate | 1e-4 | Standard for rank-16 flux LoRA |
| Steps | 1000 | 18 images × 15 repeats × ~3.7 epochs |
| Optimizer | AdamW8bit | Memory efficient, stable |
| Precision | bf16 | Best for Ampere/Ada with fp8 flux |
| Resolution | 1024 | Match ComfyUI generation resolution |
| Num repeats | 15 | Compensates for small dataset |

---

## Character Trigger Words

| Character | Trigger Word | Slug |
|-----------|-------------|------|
| James Osei | `JAMES_OSEI_EB` | james-osei |
| Lena Hartmann | `LENA_HARTMANN_EB` | lena-hartmann |
| Tom Westhall | `TOM_WESTHALL_EB` | tom-westhall |
| Rosa Mendez | `ROSA_MENDEZ_EB` | rosa-mendez |
| Mei Lin | `MEI_LIN_EB` | mei-lin |
| Elena Vasquez | `ELENA_VASQUEZ_EB` | elena-vasquez |
| Amara Diallo | `AMARA_DIALLO_EB` | amara-diallo |
| Marcus Webb | `MARCUS_WEBB_EB` | marcus-webb |
| Kenji Nakamura | `KENJI_NAKAMURA_EB` | kenji-nakamura |
| Priya Sharma | `PRIYA_SHARMA_EB` | priya-sharma |
| Dara Okonkwo | `DARA_OKONKWO_EB` | dara-okonkwo |
| Sam Torres | `SAM_TORRES_EB` | sam-torres |

---

## Caption Format

Captions are in Kohya SS format: trigger word first, then scene description.

Example (`EB-fp-01-social media-T1-chars-James Osei-01_00001_.txt`):
```
JAMES_OSEI_EB, full-length documentary portrait, Kumasi Ghana hempcrete construction site, afternoon West African light, practical work clothes, confident direct gaze, red laterite earth, 35mm documentary photography
```

The character identity (age, ethnicity, features) is intentionally omitted from captions — that's what the trigger word will learn to encode from the images themselves.

---

## Curating Images Before Training

After `prepare-dataset.py` runs, do a quick curation pass:

1. Open `datasets/james-osei/images/` in Explorer
2. Delete any image that: shows wrong face, bad anatomy, wrong location, or is a near-duplicate
3. You need minimum 10 good images per character; 15+ is ideal
4. The matching `.txt` file will be automatically skipped (Kohya ignores unpaired files)

---

## If Training Fails

Common issues:

**Out of memory:** Add to config: `cache_latents = true`, `gradient_checkpointing = true` (already set)

**Wrong trigger word appearing in outputs:** Increase `keep_tokens = 1` (already set)

**LoRA not affecting the character:** Increase LoRA strength in ComfyUI (try 1.2), or increase training steps

**Loss not converging:** Lower LR to `5e-5` in the TOML config
